{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I06AD-l-0NJY"
      },
      "source": [
        "**Objective: - Image Classification with CIFAR 100**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryws2TN0zvWa"
      },
      "source": [
        "This dataset is just like the CIFAR-10, except it has 100 classes containing 600\n",
        "images each. There are 500 training images and 100 testing images per class.\n",
        "The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image\n",
        "comes with a &quot;fine&quot; label (the class to which it belongs) and a &quot;coarse&quot; label\n",
        "(the superclass to which it belongs).\n",
        "\n",
        "Dataset Link: - https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "The dataset is not direct images. Please decode it using your own techniques.\n",
        "\n",
        "**Task: - Create a Web Application using Flask. Use the end user should be able\n",
        "to upload an image and get results with the prediction score.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCp2g9Oz0W2m"
      },
      "source": [
        "To create a web application using Flask for image classification with the CIFAR-100 dataset, follow these steps:\n",
        "\n",
        "Load and pre-process the CIFAR-100 dataset. This includes decoding the images, splitting into training and testing sets, and possibly transforming or normalizing the data.\n",
        "\n",
        "Train a deep learning model on the training data. This could be a convolutional neural network (CNN) or another type of image classification model.\n",
        "\n",
        "Evaluate the model on the test data and save the model weights.\n",
        "\n",
        "Set up a Flask web server and create the HTML templates for the user interface. This includes the form for uploading an image and displaying the prediction results.\n",
        "\n",
        "Write the code for the prediction endpoint in Flask that takes an uploaded image, pre-processes it, and feeds it through the saved model to get the prediction scores for each class.\n",
        "\n",
        "Finally, test the web application to ensure it is working as expected and make any necessary improvements or bug fixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cSgvp9jzuTrN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "%matplotlib inline\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, GlobalAveragePooling2D \n",
        "from keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import albumentations as albu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the CIFAR-100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz5ILkaXwwJJ",
        "outputId": "34f50538-3371-42f6-ba2f-fcb9e892b33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-07 12:16:49--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  43.3MB/s    in 3.8s    \n",
            "\n",
            "2023-02-07 12:16:53 (42.4 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "cifar-100-python/\n",
            "cifar-100-python/file.txt~\n",
            "cifar-100-python/train\n",
            "cifar-100-python/test\n",
            "cifar-100-python/meta\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar xzvf cifar-100-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to open the files in the Python version of the dataset\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='latin1')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = r'E:\\STUDIES\\data_science\\ML\\Deep_Learning\\DL_PRACTICALS\\cifar-100-python\\cifar-100-python\\train'\n",
        "train_data = unpickle(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filenames <class 'list'>\n",
            "batch_label <class 'str'>\n",
            "fine_labels <class 'list'>\n",
            "coarse_labels <class 'list'>\n",
            "data <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "#type of items in each file\n",
        "for item in train_data:\n",
        "    print(item, type(train_data[item]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n",
            "3072\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data['data']))\n",
        "print(len(train_data['data'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
            " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
            " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
            " 96 97 98 99]\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(train_data['fine_labels']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(train_data['coarse_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training batch 1 of 1\n"
          ]
        }
      ],
      "source": [
        "print(train_data['batch_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = r'E:\\STUDIES\\data_science\\ML\\Deep_Learning\\DL_PRACTICALS\\cifar-100-python\\cifar-100-python\\test'\n",
        "test_data = unpickle(file)\n",
        "#testData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = r'E:\\STUDIES\\data_science\\ML\\Deep_Learning\\DL_PRACTICALS\\cifar-100-python\\cifar-100-python\\meta'\n",
        "meta_data = unpickle(file)\n",
        "#metaData"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Meta file has a dictionary of fine labels and coarse labels.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SuperClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aquatic_mammals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>flowers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>food_containers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fruit_and_vegetables</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>household_electrical_devices</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>household_furniture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>insects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>large_carnivores</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>large_man-made_outdoor_things</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>large_natural_outdoor_scenes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>large_omnivores_and_herbivores</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>medium_mammals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>non-insect_invertebrates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>reptiles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>small_mammals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>trees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>vehicles_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>vehicles_2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        SuperClass\n",
              "0                  aquatic_mammals\n",
              "1                             fish\n",
              "2                          flowers\n",
              "3                  food_containers\n",
              "4             fruit_and_vegetables\n",
              "5     household_electrical_devices\n",
              "6              household_furniture\n",
              "7                          insects\n",
              "8                 large_carnivores\n",
              "9    large_man-made_outdoor_things\n",
              "10    large_natural_outdoor_scenes\n",
              "11  large_omnivores_and_herbivores\n",
              "12                  medium_mammals\n",
              "13        non-insect_invertebrates\n",
              "14                          people\n",
              "15                        reptiles\n",
              "16                   small_mammals\n",
              "17                           trees\n",
              "18                      vehicles_1\n",
              "19                      vehicles_2"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#storing coarse labels along with its number code in a dataframe\n",
        "category = pd.DataFrame(meta_data['coarse_label_names'], columns=['SuperClass'])\n",
        "category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SubClass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aquarium_fish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>baby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>beaver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>whale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>willow_tree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>wolf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>worm</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         SubClass\n",
              "0           apple\n",
              "1   aquarium_fish\n",
              "2            baby\n",
              "3            bear\n",
              "4          beaver\n",
              "..            ...\n",
              "95          whale\n",
              "96    willow_tree\n",
              "97           wolf\n",
              "98          woman\n",
              "99           worm\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#storing fine labels along with its number code in a dataframe\n",
        "subCategory = pd.DataFrame(meta_data['fine_label_names'], columns=['SubClass'])\n",
        "subCategory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[255, 255, 255, ...,  10,  59,  79],\n",
              "       [255, 253, 253, ..., 253, 253, 255],\n",
              "       [250, 248, 247, ..., 194, 207, 228],\n",
              "       ...,\n",
              "       [248, 240, 236, ..., 180, 174, 205],\n",
              "       [156, 151, 151, ..., 114, 107, 126],\n",
              "       [ 31,  30,  31, ...,  72,  69,  67]], dtype=uint8)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = train_data['data']\n",
        "X_train"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Transformation for Tensorflow (Keras) and Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#4D array input for building the CNN model using Keras\n",
        "X_train = X_train.reshape(len(X_train),3,32,32).transpose(0,2,3,1)\n",
        "#X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#transforming the testing dataset\n",
        "X_test = test_data['data']\n",
        "X_test = X_test.reshape(len(X_test),3,32,32).transpose(0,2,3,1)\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = train_data['fine_labels']\n",
        "#y_train\n",
        "\n",
        "y_test = test_data['fine_labels']\n",
        "#y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CONVERTING CLASS VECTORS TO BINARY CLASS MATRICES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_classes = 100\n",
        "\n",
        "y_train = to_categorical(y_train, n_classes)\n",
        "#y_train\n",
        "\n",
        "y_test = to_categorical(y_test, n_classes)\n",
        "#y_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EXPLORING THE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image number selected : 38201\n",
            "Shape of image : (32, 32, 3)\n",
            "Image category number: 11\n",
            "Image category name: Large_omnivores_and_herbivores\n",
            "Image subcategory number: 21\n",
            "Image subcategory name: Chimpanzee\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#generating a random number to display a random image from the dataset along with the label's number and name\n",
        "\n",
        "rcParams['figure.figsize'] = 2,2\n",
        "\n",
        "imageId = np.random.randint(0, len(X_train))\n",
        "\n",
        "plt.imshow(X_train[imageId])\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "print(\"Image number selected : {}\".format(imageId))\n",
        "print(\"Shape of image : {}\".format(X_train[imageId].shape))\n",
        "print(\"Image category number: {}\".format(train_data['coarse_labels'][imageId]))\n",
        "print(\"Image category name: {}\".format(category.iloc[train_data['coarse_labels'][imageId]][0].capitalize()))\n",
        "print(\"Image subcategory number: {}\".format(train_data['fine_labels'][imageId]))\n",
        "print(\"Image subcategory name: {}\".format(subCategory.iloc[train_data['fine_labels'][imageId]][0].capitalize()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples:  40000\n",
            "Number of validation samples:  10000\n"
          ]
        }
      ],
      "source": [
        "#using stratified shuffle split to preserve the percentage of samples in each of the 100 classes\n",
        "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=123)\n",
        "\n",
        "for train_index, val_index in sss.split(X_train, y_train):\n",
        "    X_train_data, X_val_data = X_train[train_index], X_train[val_index]\n",
        "    y_train_data, y_val_data = y_train[train_index], y_train[val_index]\n",
        "\n",
        "print(\"Number of training samples: \", X_train_data.shape[0])\n",
        "print(\"Number of validation samples: \", X_val_data.shape[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CONSTANTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#resizing the images as per EfficientNetB5 to size (456, 456)\n",
        "height = 224\n",
        "width = 224\n",
        "channels = 3\n",
        "\n",
        "n_classes = 100\n",
        "input_shape = (height, width, channels)\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 8"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**resize function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize_img(img, shape):\n",
        "    return cv2.resize(img, (shape[1], shape[0]), interpolation=cv2.INTER_CUBIC)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Custom Data Generator class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, images, labels=None, mode='fit', batch_size=batch_size, dim=(height, width), channels=channels, n_classes=n_classes, shuffle=True, augment=False):\n",
        "        \n",
        "        #initializing the configuration of the generator\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.channels = channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.on_epoch_end()\n",
        "   \n",
        "    #method to be called after every epoch\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(self.images.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "    \n",
        "    #return numbers of steps in an epoch using samples and batch size\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.images) / self.batch_size))\n",
        "    \n",
        "     #this method is called with the batch number as an argument to obtain a given batch of data\n",
        "    def __getitem__(self, index):\n",
        "        #generate one batch of data\n",
        "        #generate indexes of batch\n",
        "        batch_indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
        "        \n",
        "        #generate mini-batch of X\n",
        "        X = np.empty((self.batch_size, *self.dim, self.channels))\n",
        "        \n",
        "        for i, ID in enumerate(batch_indexes):\n",
        "            #generate pre-processed image\n",
        "            img = self.images[ID]\n",
        "            #image rescaling\n",
        "            img = img.astype(np.float32)/255.\n",
        "            #resizing as per new dimensions\n",
        "            img = resize_img(img, self.dim)\n",
        "            X[i] = img\n",
        "            \n",
        "        #generate mini-batch of y\n",
        "        if self.mode == 'fit':\n",
        "            y = self.labels[batch_indexes]\n",
        "            \n",
        "            #augmentation on the training dataset\n",
        "            if self.augment == True:\n",
        "                X = self.__augment_batch(X)\n",
        "            return X, y\n",
        "        \n",
        "        elif self.mode == 'predict':\n",
        "            return X\n",
        "        \n",
        "        else:\n",
        "            raise AttributeError(\"The mode should be set to either 'fit' or 'predict'.\")\n",
        "        \n",
        "    \n",
        "     #augmentation for one image\n",
        "    def __random_transform(self, img):\n",
        "        composition = albu.Compose([albu.HorizontalFlip(p=0.5),\n",
        "                                   albu.VerticalFlip(p=0.5),\n",
        "                                   albu.GridDistortion(p=0.2),\n",
        "                                   albu.ElasticTransform(p=0.2)])\n",
        "        return composition(image=img)['image']\n",
        "    \n",
        "    #augmentation for batch of images\n",
        "    def __augment_batch(self, img_batch):\n",
        "        for i in range(img_batch.shape[0]):\n",
        "            img_batch[i] = self.__random_transform(img_batch[i])\n",
        "        return img_batch\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_generator = DataGenerator(X_train_data, y_train_data, augment=True)\n",
        "valid_data_generator = DataGenerator(X_val_data, y_val_data, augment=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Using pre-trained EfficientNetB5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import efficientnet.keras as efn\n",
        "\n",
        "efnb0 = efn.EfficientNetB5(weights='imagenet', include_top=False, input_shape=input_shape, classes=n_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(efnb0)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = Adam(lr=0.0001)\n",
        "\n",
        "#early stopping to monitor the validation loss and avoid overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "#reducing learning rate on plateau\n",
        "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 5, factor= 0.5, min_lr= 1e-6, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model compiling\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_history = model.fit_generator(train_data_generator,\n",
        "                                    validation_data=valid_data_generator,\n",
        "                                    callbacks=[early_stop, rlrop],\n",
        "                                    verbose=1,\n",
        "                                    epochs=epochs)\n",
        "\n",
        "#saving the trained model weights as data file in .h5 format\n",
        "model.save_weights(\"cifar_efficientnetB5_weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#saving the trained model as data file in .h5 format\n",
        "model.save('cifar_efficientnetb0_model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_loss, valid_accuracy = model.evaluate_generator(generator=valid_data_generator, verbose=1)\n",
        "\n",
        "print('Validation Accuracy: ', round((valid_accuracy * 100), 2), \"%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict_generator(DataGenerator(X_test, mode='predict', augment=False, shuffle=False), verbose=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "test_accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
        "\n",
        "print('Test Accuracy: ', round((test_accuracy * 100), 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot to visualize the loss and accuracy against number of epochs\n",
        "plt.figure(figsize=(18,8))\n",
        "\n",
        "plt.suptitle('Loss and Accuracy Plots', fontsize=18)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(model_history.history['loss'], label='Training Loss')\n",
        "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of epochs', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(model_history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#report to see which category has been predicted incorectly and which has been predicted correctly\n",
        "target = [\"Category {}\".format(i) for i in range(n_classes)]\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataframe of predictions\n",
        "prediction = pd.DataFrame(y_pred)\n",
        "prediction.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#generating a random number to display a random image from the dataset along with the true and predicted label\n",
        "imageId = np.random.randint(0, len(X_test))\n",
        "\n",
        "rcParams['figure.figsize'] = 2,2\n",
        "\n",
        "plt.imshow(X_test[imageId])\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "print(\"True Label: \" + str(subCategory.iloc[test_data['fine_labels'][imageId]][0].capitalize()))\n",
        "print(\"Predicted Label: \" + str(subCategory.iloc[prediction.iloc[imageId]]).split()[2].capitalize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#16 random images to display at a time along with their true and random labels\n",
        "rcParams['figure.figsize'] = 12,15\n",
        "\n",
        "num_row = 4\n",
        "num_col = 4\n",
        "\n",
        "imageId = np.random.randint(0, len(X_test), num_row * num_col)\n",
        "\n",
        "fig, axes = plt.subplots(num_row, num_col)\n",
        "\n",
        "for i in range(0, num_row):\n",
        "    for j in range(0, num_col):\n",
        "        k = (i*num_col)+j\n",
        "        axes[i,j].imshow(X_test[imageId[k]])\n",
        "        axes[i,j].set_title(\"True: \" + str(subCategory.iloc[testData['fine_labels'][imageId[k]]][0]).capitalize() \n",
        "                             + \"\\nPredicted: \" + str(subCategory.iloc[prediction.iloc[imageId[k]]]).split()[2].capitalize(), \n",
        "                            fontsize=14)\n",
        "        axes[i,j].axis('off')\n",
        "        fig.suptitle(\"Images with True and Predicted Labels\", fontsize=18) \n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to resize the image\n",
        "def resize_test_image(test_img):\n",
        "\n",
        "    img = cv2.imread(test_img)\n",
        "    #plt.imshow(img)\n",
        "    img_RGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    #plt.imshow(img_RGB)\n",
        "    resized_img = cv2.resize(img_RGB, (456, 456))\n",
        "    #plt.imshow(resized_img)\n",
        "    resized_img = resized_img / 255.\n",
        "    #plt.imshow(resized_img)\n",
        "    return resized_img\n",
        "    \n",
        "#resize_test_image('orange.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to get prediction for test image from the model\n",
        "def predict_test_image(test_img):\n",
        "    \n",
        "    resized_img = resize_test_image(test_img)\n",
        "    prediction = model.predict(np.array([resized_img]))\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "#predict_test_image('orange.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to get the sorted prediction\n",
        "def sort_prediction_test_image(test_img):\n",
        "    \n",
        "    prediction = predict_test_image(test_img)\n",
        "    \n",
        "    index = np.arange(0,100)\n",
        "    \n",
        "    for i in range(100):\n",
        "        for j in range(100):\n",
        "            if prediction[0][index[i]] > prediction[0][index[j]]:\n",
        "                temp = index[i]\n",
        "                index[i] = index[j]\n",
        "                index[j] = temp\n",
        "                \n",
        "    return index\n",
        "\n",
        "#sort_prediction_test_image('orange.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to get the dataframe for top 5 predictions\n",
        "def df_top5_prediction_test_image(test_img):\n",
        "    \n",
        "    sorted_index = sort_prediction_test_image(test_img)\n",
        "    prediction = predict_test_image(test_img)\n",
        "    \n",
        "    subCategory_name = []\n",
        "    prediction_score = []\n",
        "    \n",
        "    k = sorted_index[:6] \n",
        "    \n",
        "    for i in range(len(k)):\n",
        "        subCategory_name.append(subCategory.iloc[k[i]][0])\n",
        "        prediction_score.append(round(prediction[0][k[i]], 2))\n",
        "        \n",
        "    df = pd.DataFrame(list(zip(subCategory_name, prediction_score)), columns=['Label', 'Probability'])  \n",
        "    \n",
        "    return df\n",
        "\n",
        "#df_top5_prediction_test_image('.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to get the plot for top 5 predictions \n",
        "def plot_top5_prediction_test_image(test_img):\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,4))\n",
        "    fig.suptitle(\"Prediction\", fontsize=18)\n",
        "    \n",
        "    new_img = plt.imread(test_img)\n",
        "    axes[0].imshow(new_img)\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    data = df_top5_prediction_test_image(test_img)\n",
        "    x=df_top5_prediction_test_image(test_img)['Label']\n",
        "    y=df_top5_prediction_test_image(test_img)['Probability']\n",
        "    \n",
        "    axes[1] = sns.barplot(x=x, y=y, data=data, color=\"green\")\n",
        "    \n",
        "    plt.xlabel('Label', fontsize=14)\n",
        "    plt.ylabel('Probability', fontsize=14)\n",
        "    \n",
        "    plt.ylim(0,1.0)\n",
        "    \n",
        "    axes[1].grid(False)\n",
        "    axes[1].spines[\"top\"].set_visible(False)\n",
        "    axes[1].spines[\"right\"].set_visible(False)\n",
        "    axes[1].spines[\"bottom\"].set_visible(False)\n",
        "    axes[1].spines[\"left\"].set_visible(False)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_top5_prediction_test_image('orange.jpeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1y1SadX0jZ0"
      },
      "source": [
        "This code loads the CIFAR-100 dataset using the pickle library, which allows you to load Python objects saved in binary format. The data is then split into training and test sets. The images are reshaped from flat arrays to 3D arrays with dimensions (num_samples, 32, 32, 3) and scaled to values between 0 and 1. Finally, the labels are one-hot encoded to prepare them for use with a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBNr3c_M0bC7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cifar100",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "fe438a21af38e17a9e3f5b2e4e1554680bdba9e77e0ce550e42c34514bf67863"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
